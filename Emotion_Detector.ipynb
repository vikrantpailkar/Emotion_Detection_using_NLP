{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ba6fa029deab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifiers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaivebayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validation'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import nltk \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from nltk.classify.naivebayes import NaiveBayesClassifier\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.corpus import wordnet as wn\n",
    "import urllib\n",
    "import urllib2\n",
    "# Uncomment if using AlchemyAPI\n",
    "# from alchemyapi import AlchemyAPI\n",
    "# alchemyapi = AlchemyAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reading the Dataset (ISEAR Dataset)\n",
    "'''\n",
    "Data = pd.read_csv('ISEAR.csv',header=None)\n",
    "'''\n",
    "36 - Class Label\n",
    "40 - Sentence\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Emotion Labels\n",
    "'''\n",
    "emotion_labels = ['joy', 'fear', 'anger', 'sadness', 'disgust', 'shame', 'guilt']\n",
    "# emotion_labels = ['joy', 'fear', 'anger', 'sadness', 'disgust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Negation words\n",
    "'''\n",
    "negation_words = ['not', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except', 'even though', 'yet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns a list of all corresponding class labels\n",
    "'''\n",
    "def class_labels(emotions):\n",
    "    labels = []\n",
    "    labelset = []\n",
    "    exclude = []\n",
    "    for i in range(len(emotions)):\n",
    "#         labels.append(e)\n",
    "#         labelset.append([e])\n",
    "        if emotions[i] not in ['shame','guilt']:\n",
    "            labels.append(e)\n",
    "            labelset.append([e])\n",
    "        else:\n",
    "            exclude.append(i)\n",
    "    return labels, labelset, exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Removes unnecessary characters from sentences\n",
    "'''\n",
    "def removal(sentences):\n",
    "    sentence_list = []\n",
    "    count = 0\n",
    "#     for sen in sentences:\n",
    "#         count += 1\n",
    "#         print count\n",
    "#         print sen\n",
    "#         print type(sen)\n",
    "    s = nltk.word_tokenize(sentences)\n",
    "    characters = [\"รก\", \"\\xc3\", \"\\xa1\", \"\\n\", \",\", \".\", \"[\", \"]\", \"\"]\n",
    "    l = []\n",
    "    for t in s:\n",
    "        if t not in characters:\n",
    "            l.append(t)\n",
    "    return l\n",
    "#     new = ' '.join([i for i in s if not [e for e in characters if e in i]])\n",
    "#     print new\n",
    "#     sentence_list.append(new)\n",
    "#     return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "POS-TAGGER, returns NAVA words\n",
    "'''\n",
    "def pos_tag(sentences):\n",
    "    tags = [] #have the pos tag included\n",
    "    nava_sen = []\n",
    "    pt = nltk.pos_tag(sentences)\n",
    "#     for s in sentences:\n",
    "#     s_token = nltk.word_tokenize(sentences)\n",
    "#     pt = nltk.pos_tag(s_token)\n",
    "    nava = []\n",
    "    nava_words = []\n",
    "    for t in pt:\n",
    "        if t[1].startswith('NN') or t[1].startswith('JJ') or t[1].startswith('VB') or t[1].startswith('RB'):\n",
    "            nava.append(t)\n",
    "            nava_words.append(t[0])\n",
    "    return nava, nava_words\n",
    "#     tags.append(nava)\n",
    "#     nava_sen.append(nava_words)\n",
    "#     return tags, nava_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Performs stemming\n",
    "'''\n",
    "def stemming(sentences):\n",
    "    sentence_list = []\n",
    "    sen_string = []\n",
    "    sen_token = []\n",
    "    stemmer = PorterStemmer()\n",
    "    i = 0\n",
    "#     for sen in sentences:\n",
    "#         print i,\n",
    "    i += 1\n",
    "    st = \"\"\n",
    "    for word in sentences:\n",
    "        word_l = word.lower()\n",
    "        if len(word_l) >= 3:\n",
    "            st += stemmer.stem(word_l) + \" \"\n",
    "    sen_string.append(st)\n",
    "    w_set = nltk.word_tokenize(st)\n",
    "    sen_token.append(w_set)\n",
    "    w_text = nltk.Text(w_set)\n",
    "    sentence_list.append(w_text)\n",
    "    return w_text, st, w_set\n",
    "#     return sentence_list, sen_string, sen_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write to file\n",
    "'''\n",
    "def write_to_file(filename, text):\n",
    "    o = open(filename,'w')\n",
    "    o.write(str(text))\n",
    "    o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating the dataframe\n",
    "'''\n",
    "def create_frame(Data):\n",
    "    labels = []\n",
    "#     sentences = []\n",
    "#     sen_string = []\n",
    "#     sen_token =[]\n",
    "    sen = []\n",
    "    sen_s = []\n",
    "    sen_t = []\n",
    "    labelset = []\n",
    "    for i in range(len(Data)):\n",
    "        if i >= 0:\n",
    "#             print i,\n",
    "            emotion = Data[0][i]\n",
    "            sit = Data[1][i]\n",
    "#             if emotion not in ['shame', 'guilt']:\n",
    "            labels.append(emotion)\n",
    "            labelset.append([emotion])\n",
    "            sent = removal(sit)\n",
    "            nava, sent_pt = pos_tag(sent)\n",
    "            sentences, sen_string, sen_token = stemming(sent_pt)\n",
    "            sen.append(sentences)\n",
    "            sen_s.append(sen_string)\n",
    "            sen_t.append(sen_token)\n",
    "#     labels, labelset, exclude = class_labels(emotions[1:])\n",
    "#     sent = removal(sit[1:], exclude)\n",
    "#     nava, sent_pt = pos_tag(sent)\n",
    "#     sentences, sen_string, sen_token = stemming(sent_pt)\n",
    "    frame = pd.DataFrame({0 : labels,\n",
    "                          1 : sen,\n",
    "                          2 : sen_s,\n",
    "                          3 : sen_t,\n",
    "                          4 : labelset})\n",
    "    return frame, sen_t, labels, sen_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, st, labels, senten = create_frame(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reads the emotion representative words file\n",
    "'''\n",
    "def readfile(filename):\n",
    "    f = open(filename,'r')\n",
    "    representative_words = []\n",
    "    for line in f.readlines():\n",
    "        characters = [\"\\n\", \" \", \"\\r\", \"\\t\"]\n",
    "        new = ''.join([i for i in line if not [e for e in characters if e in i]])\n",
    "        representative_words.append(new)\n",
    "    return representative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Makes a list of all words semantically related to an emotion and Stemming\n",
    "'''\n",
    "def affect_wordlist(words):\n",
    "    affect_words = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for w in words:\n",
    "        w_l = w.lower()\n",
    "        word_stem = stemmer.stem(w_l)\n",
    "        if word_stem not in affect_words:\n",
    "            affect_words.append(word_stem)\n",
    "    return affect_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating an emotion wordnet\n",
    "'''\n",
    "def emotion_word_set(emotions):\n",
    "    word_set = {}\n",
    "    for e in emotions:\n",
    "        representative_words = readfile(e)\n",
    "        wordlist = affect_wordlist(representative_words)\n",
    "        word_set[e] = wordlist\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Lexicon based approach - Check for lexicons\n",
    "'''\n",
    "def lexicon_based(sentences, word_set):\n",
    "    text_vector = []\n",
    "    for sen in sentences:\n",
    "        s_vector = []\n",
    "        for word in sen:\n",
    "            w_vector = {}\n",
    "            for emo in word_set:\n",
    "                if word in word_set[emo]:\n",
    "#                     print word\n",
    "                    try:\n",
    "                        if emo not in w_vector[word]:\n",
    "                            w_vector[word].append(emo)\n",
    "                    except KeyError:\n",
    "                        w_vector[word] = [emo]\n",
    "            if w_vector:\n",
    "                s_vector.append(w_vector)\n",
    "        if not s_vector:\n",
    "            text_vector.append(s_vector)\n",
    "        else:\n",
    "            text_vector.append(s_vector)\n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Lexicon based approach - Classify based on lexicons\n",
    "'''\n",
    "def classify_lexicon(text_vector, labels, emotion_labels):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for j in range(len(text_vector)):\n",
    "        sen = text_vector[j]\n",
    "        sen_emo = np.empty(len(emotion_labels))\n",
    "        sen_emo.fill(0)\n",
    "        if sen:\n",
    "            total += 1\n",
    "            w_emo = []\n",
    "            for word in sen:\n",
    "                emotions =  word.values()[0][0]\n",
    "#                 print emotions, type(emotions), j\n",
    "                w_emo.append(emotions)\n",
    "                i = emotion_labels.index(emotions)\n",
    "                sen_emo[i] += 1\n",
    "#             print sen_emo\n",
    "            winner = np.argwhere(sen_emo == np.amax(sen_emo))\n",
    "            indices = winner.flatten().tolist()\n",
    "            for i in indices:\n",
    "                if emotion_labels[i] == labels[j]:\n",
    "                    count += 1\n",
    "                    break\n",
    "#                 else:\n",
    "#                     print j, text_vector[j]\n",
    "    accuracy = count/len(text_vector)\n",
    "    tot_accuracy = count/total\n",
    "    return accuracy, tot_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = emotion_word_set(emotion_labels)\n",
    "l = lexicon_based(c[1],e) \n",
    "a, b = classify_lexicon(l, c[0], emotion_labels)\n",
    "print a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate pmi\n",
    "'''\n",
    "def pmi(x, y, sentences):\n",
    "    count_x = 1\n",
    "    count_y = 1\n",
    "    count_xy = 1\n",
    "    for sen in sentences:\n",
    "        if x and y in sentences:\n",
    "            count_xy += 1\n",
    "            count_x += 1\n",
    "            count_y += 1\n",
    "        if x in sentences:\n",
    "            count_x += 1\n",
    "        if y in sentences:\n",
    "            count_y += 1\n",
    "        result = count_xy/(count_x * count_y)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print a*100, '%'\n",
    "print b*100, \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Emotion Detector\n",
    "'''\n",
    "# c = create_frame(Data)\n",
    "# emo_word_net = emotion_word_set(emotion_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Getting synonyms from wordnet synsets\n",
    "'''\n",
    "def get_synonyms():\n",
    "    syn = {}\n",
    "    for e in emotion_labels:\n",
    "        jw = wn.synsets(e)\n",
    "        for s in jw:\n",
    "            v = s.name()\n",
    "            try:\n",
    "                syn[e].append(wn.synset(v).lemma_names())\n",
    "            except KeyError:\n",
    "                syn[e] = wn.synset(v).lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating training/testing set for Naive Bayes classifier TextBlob -- Not used\n",
    "'''\n",
    "def create_dataset_textblob(sentences, emotions):\n",
    "    train = []\n",
    "    sen = []\n",
    "    emo = []\n",
    "    for s in sentences:\n",
    "        sen.append(s)\n",
    "    for e in emotions:\n",
    "        emo.append(e)\n",
    "    for i in range(len(sen)):\n",
    "        s = sen[i]\n",
    "        e = emo[i]\n",
    "        train.append((str(s), e))\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Testing for Naive Bayes Classifier\n",
    "'''\n",
    "def testing(cl, test):\n",
    "    for s, e in test:\n",
    "        r = cl.classify(s)\n",
    "        print s, e, r\n",
    "        if r == e:\n",
    "            print \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create dataset for nltk Naive Bayes\n",
    "'''\n",
    "def create_data(sentence, emotion):\n",
    "    data = []\n",
    "    for i in range(len(sentence)):\n",
    "        sen = []\n",
    "        for s in sentence[i]:\n",
    "            sen.append(str(s))\n",
    "        emo = emotion[i]\n",
    "        data.append((sen, emo))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get all words in dataset\n",
    "'''\n",
    "def get_words_in_dataset(dataset):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in dataset:\n",
    "        all_words.extend(words)\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Getting frequency dist of words\n",
    "'''\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Extacting features\n",
    "'''\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create test data\n",
    "'''\n",
    "def create_test(sentence, emotion):\n",
    "    data = []\n",
    "    sen = []\n",
    "    emo = []\n",
    "    for s in sentence:\n",
    "        sen.append(str(s))\n",
    "    for e in emotion:\n",
    "        emo.append(e)\n",
    "    for i in range(len(sen)):\n",
    "        temp = []\n",
    "        temp.append(sen[i])\n",
    "        temp.append(emo[i])\n",
    "        data.append(temp)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Classifier\n",
    "'''\n",
    "def classify_dataset(data):\n",
    "    return \\\n",
    "        classifier.classify(extract_features(nltk.word_tokenize(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get accuracy\n",
    "'''\n",
    "def get_accuracy(test_data, classifier):\n",
    "    total = accuracy = float(len(test_data))\n",
    "    for data in test_data:\n",
    "        if classify_dataset(data[0]) != data[1]:\n",
    "#             print data, classify_dataset(data[0]), data[1]\n",
    "            accuracy -= 1\n",
    "    print('Total accuracy: %f%% (%d/20).' % (accuracy / total * 100, accuracy))\n",
    "    final = accuracy / total * 100\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing data\n",
    "sen = c[3]\n",
    "emo = c[0]\n",
    "l = len(c[3])\n",
    "limit = (9*l)//10\n",
    "sente = c[2]\n",
    "Data = create_data(sen[:limit], emo[:limit])\n",
    "test_data = create_test(sente[limit:], emo[limit:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the word features out from the training data\n",
    "word_features = get_word_features(\\\n",
    "                    get_words_in_dataset(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training set and train the Naive Bayes Classifier\n",
    "training_set = nltk.classify.util.apply_features(extract_features, Data)\n",
    "classifier = NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive_accu = get_accuracy(test_data, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(st)\n",
    "train_size = (9 * length) // 10\n",
    "train_data = st[:train_size]\n",
    "test_data = senten[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_dict = corpora.Dictionary(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_dict.save('corpus.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_token = gensim_dict.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusmm = [gensim_dict.doc2bow(text) for text in train_data]\n",
    "corpora.MmCorpus.serialize('corpus.mm', corpusmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_c = corpora.MmCorpus('corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(gen_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[gen_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load('corpus.dict')\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=gensim_dict, num_topics=7) # initialize an LSI transformation\n",
    "corpus_lsi = lsi[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[gen_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi.save('corpus.lsi') \n",
    "index.save('corpus.index')\n",
    "# lsi = models.LsiModel.load('corpus.lsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length = len(corpus_lsi)\n",
    "# train_size = (9 * length) // 10\n",
    "# train_data = corpus_lsi[:train_size]\n",
    "# test_data = corpus_lsi[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = labels[train_size:]\n",
    "train_labels = labels[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_sim(test_data):\n",
    "    total = 0\n",
    "    for count, doc in enumerate(test_data):\n",
    "        vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "        vec_lsi = lsi[vec_bow]\n",
    "        sims = index[vec_lsi]\n",
    "        ans = sorted(enumerate(sims), key=lambda item: -item[1])[0]\n",
    "        ind = ans[0] \n",
    "        if train_labels[ind] == test_labels[count]:\n",
    "            total += 2\n",
    "    avg = total/count\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_accu = semantic_sim(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with Alchemy API\n",
    "url = 'http://gateway-a.watsonplatform.net/calls/text/TextGetEmotion'\n",
    "params = urllib.urlencode({\n",
    "  'apikey': '15ce4bd07b66f9e000a15383777870c0afb383fb',\n",
    "  'text': 'I am excited',\n",
    "  'outputMode': 'json'\n",
    "})\n",
    "response = urllib2.urlopen(url, params).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Accuracy using Lexical Component - First Phase  \", a*100, '%'\n",
    "print \"Accuracy using Lexical Component - Second Phase  \", b*100, \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Accuracy using Naive Bayes Component  \", Naive_accu, \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Accuracy using semantic similarity component \", sem_accu*100, \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
